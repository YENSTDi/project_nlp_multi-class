{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d9671cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import logit, nn\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f469f5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurModel(nn.Module):\n",
    "    def __init__(self, ori_model, num_labels):\n",
    "        super(OurModel, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        self.model = model = ori_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(768, num_labels)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        layers = self.dropout(outputs[0])\n",
    "        logits = self.classifier(layers[:, 0, :].view(-1, 768))\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "          loss_fct = nn.CrossEntropyLoss()\n",
    "          loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        return TokenClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states,attentions=outputs.attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e88e6465",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at uer/roberta-base-finetuned-jd-full-chinese were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "NAME = \"uer/roberta-base-finetuned-jd-full-chinese\"\n",
    "model = OurModel(AutoModel.from_pretrained(NAME), 32)\n",
    "checkpoint = torch.load('multi-class-model', map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "tokenizer = AutoTokenizer.from_pretrained(NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a16eef09",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('testdata.csv')\n",
    "df = data.sample(frac=1).iloc[0]\n",
    "x = df['content']\n",
    "y = df['reCheckedsubject']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02e875e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(x, max_length=512, return_tensors='pt')\n",
    "logits = model(inputs['input_ids'], inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdb96913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8.9257,  0.0919,  0.4907, -1.2511, -1.1198,  0.5026, -1.2630, -1.1901,\n",
       "        -1.2066, -1.1213, -0.4370,  0.1155,  1.4614, -1.7506, -0.6827, -1.7161,\n",
       "         0.6961,  0.3379, -1.7630,  0.3730, -0.5998, -1.4802, -1.0955, -0.8299,\n",
       "        -1.1541,  1.0293, -0.0771, -0.4313, -1.8244, -1.9842, -2.1963, -0.7560],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.logits[0].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c776183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(logits.logits[0].cpu()).tolist() == y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6568365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
